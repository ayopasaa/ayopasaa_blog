[{"authors":["admin"],"categories":null,"content":"Possui Mestrado e Doutorado em Química Analítica pela Universidade Estadual de Campinas (2013-2019) e graduação em Química pela Universidad Nacional de Colombia (2011). Conhecimentos na área de química, com ênfase em química analítica e ambiental, preparo de amostras, análise fisicoquímico, cromatográfico e espectrofotomêtrico. Experiência no desenvolvimento de métodos inovadores aplicados a matrizes complexas.\nO objetivo deste site é oferecer informações e soluções através do software R para problemas de química analítica, em especial aqueles relacionados a análises de dados estatísticos, assunto negligenciado por muitos professionais que validam métodos, sendo este um tópico de relevância contemporânea que precisa de uma atenção imperativa.\nAs opiniões expressas neste site são de responsabilidade do autor e não representam necessariamente as opiniões ou políticas de sua organização/agência afiliada\n","date":1608163200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608163200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Possui Mestrado e Doutorado em Química Analítica pela Universidade Estadual de Campinas (2013-2019) e graduação em Química pela Universidad Nacional de Colombia (2011). Conhecimentos na área de química, com ênfase em química analítica e ambiental, preparo de amostras, análise fisicoquímico, cromatográfico e espectrofotomêtrico. Experiência no desenvolvimento de métodos inovadores aplicados a matrizes complexas.\nO objetivo deste site é oferecer informações e soluções através do software R para problemas de química analítica, em especial aqueles relacionados a análises de dados estatísticos, assunto negligenciado por muitos professionais que validam métodos, sendo este um tópico de relevância contemporânea que precisa de uma atenção imperativa.","tags":null,"title":"Alejandro Yopasá Arenas","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Alejandro Yopasá Arenas"],"categories":["Validação"],"content":"\r\r1 Linearidade num só clique?\r1.1 Vantagens da automatização\r\r2 Tutorial para análise da linearidade no R\r2.1 Preparação\r2.2 Importar/inserir os dados no R\r2.3 Ajuste do modelo linear\r2.4 Análise dos coeficientes do modelo linear:\r2.4.1 Intercepto\r2.4.2 Coeficiente angular\r2.4.3 Anova no método dos mínimos quadrados ordinários (MMQO)\r\r2.5 Análise de Resíduos\r2.5.1 Gráficos de diagnóstico\r2.5.2 Teste de normalidade dos resíduos\r2.5.3 Teste de homocedasticidade\r2.5.4 Teste de autocorrelação\r\r2.6 Limite de detecção e quantificação\r\r3 Detalhes adicionais sobre desenvolvimento do documento dinâmico\r3.1 Equações\r3.2 Texto\r3.3 Gráficos\r\r\r\r1 Linearidade num só clique?\rPara aqueles familiarizados com programação é que não gostam muito do Excel (Vantagens do R sobre Excel, R vs Excel) existe a opção de automatizar mais fácilmente diversos tipos de análises e relatórios. Ferramentas para o usuário mais familiarizado com o Excel já existem faz tempo é são muito confiáveis e boas, por exemplo Action Stat da Estatcamp. Recomento fortemente a revisão do Portal Action para estudo e profundização, grande parte do conteúdo deste post vem da revisão das apostilhas postadas lá junto com a revisão das normas/documentos oficiais que fazem parte do dia a dia do analista que se desenvolve na industria.\nO objetivo deste post é mostrar um exemplo de automatização de documento utilizando o ambiente R. No R Markdown é possivel automatizar a análise de linearidade para gerar um documento dinâmico em diversos formatos, como Word (o mais popular) ou HTML (este post) pronto para anexar ao relatório de validação analítica. Neste post pretendo mostrar como é a estrutura deste tipo de arquivo Rmarkdown (.Rmd) e suas capacidades na forma de tutorial.\n1.1 Vantagens da automatização\rO meu objetivo principal quando programo é economizar tempo nas minhas rotinas, sendo a análise da linearidada parte do meu trabalho eu dediquei um tempo criando um documento dinâmico que em poucos segundos gera um documento .doc o qual práticamente da para copiar e colar com mínima revisão num relatório de validação analítica. O documento dinâmico é reprodutível e possui menos erros pois o processo requer mínima intervenção humana. Como gerencio apenas o código-fonte do documento, fico livre de etapas manuais como ter que refazer um gráfico ou uma tabela após qualquer alteração na análise. Com um pouco mais de trabalho da para dinamizar incluso fórmulas e até mesmo o texto dos relatórios:\nProcesso de geração de relatório de linearidade num só clique\n\rPara um novo conjunto de dados é so dar mais um clique e gerar um novo relatório em segundos, sem ter que inserir manualmente nada pois já atualiza tabelas, gráficos, equações, textos explicativos e conclusões:\nProcesso de geração de relatório de linearidade: tabelas dinâmicas\n\rProcesso de geração de relatório de linearidade: gráficos dinâmicos\n\rProcesso de geração de relatório de linearidade: gráficos dinâmicos\n\rProcesso de geração de relatório de linearidade: fórmulas dinâmicas\n\rProcesso de geração de relatório de linearidade: texto dinâmico\n\rNa pasta no Github deste post é possível encontrar, entre outros, dados de exemplo, o arquivo .Rmd que permite gerar o relatório e relatorios de linearidade em word para visualização: Pasta com arquivos para download\n\r\r2 Tutorial para análise da linearidade no R\r2.1 Preparação\rConforme RDC 166 no Art. 25 para o estabelecimento da linearidade, deve-se utilizar, no mínimo, 5 (cinco) concentrações diferentes da SQR (substância química de referência) para soluções preparadas em, no mínimo, triplicata. Geralmente é utilizada uma planilha de validação simples para calcular as diferentes concentrações e registrar as respostas analíticas (área do pico, absorbância, etc…), neste tipo de planilha é fácil visualizar a curva e calcular o coeficiente de determininação, esta análise de linearidade aparente é importante mas não é comprobatório, sendo necessárias mais análises estatísticos.\nAnálise de linearidade aparente numa planilha de cálculo\n\rComo boa prática é bom deixar os dados no tipo de arquivo mais simples possível sem formatos, preferívelmente em texto simples: .csv, .txt, ou então mesmo como planilha de excel, no R é possivel trabalhar com diversos formatos.\nArquivo simples e tipico que contém os dados da linearidade\n\r\r2.2 Importar/inserir os dados no R\rO primeiro é definir o diretório de trabalho, i.e. onde está o arquivo contendo a nossa curva, isto se faz com a função setwd()\nsetwd(\u0026quot;C:\\\\Users\\\\Alejandro\\\\Dropbox\\\\ayopasaa_blog\\\\content\\\\post\\\\Linearidade1\u0026quot;) # vai ser diferente para cada usuário\rUtilizando a função list.files() podemos ver o conteúdo da nossa pasta de trabalho, neste exemplo os dados experimentais da linearidade estão no arquivo “dados_linearidade.csv”, esse o arquivo que precissamos importar.\nlist.files()\r## [1] \u0026quot;anova.html\u0026quot; ## [2] \u0026quot;Arquivos\u0026quot; ## [3] \u0026quot;blog-lin1.png\u0026quot; ## [4] \u0026quot;comp_formulas.PNG\u0026quot; ## [5] \u0026quot;comp_graficos.PNG\u0026quot; ## [6] \u0026quot;comp_graficos1.PNG\u0026quot; ## [7] \u0026quot;comp_tabelas.PNG\u0026quot; ## [8] \u0026quot;comp_texto.PNG\u0026quot; ## [9] \u0026quot;csvex.PNG\u0026quot; ## [10] \u0026quot;Curva1.PNG\u0026quot; ## [11] \u0026quot;dados_linearidade.csv\u0026quot; ## [12] \u0026quot;ext.PNG\u0026quot; ## [13] \u0026quot;featured.png\u0026quot; ## [14] \u0026quot;index.html\u0026quot; ## [15] \u0026quot;index.Rmd\u0026quot; ## [16] \u0026quot;index.utf8.md\u0026quot; ## [17] \u0026quot;index_cache\u0026quot; ## [18] \u0026quot;index_files\u0026quot; ## [19] \u0026quot;lin_hetero.csv\u0026quot; ## [20] \u0026quot;linearidade_AYA.html\u0026quot; ## [21] \u0026quot;linearidade_hetero_portalaction.html\u0026quot;\rNeste exemplo a nossa curva está num arquivo delimitado por comas (.csv), logo utilizamos a função read.csv2() para importar os dados é deixar eles num objeto (data.frame) que pode ter cuaisquer nome, neste caso optei por “curva_ex”\ncurva_ex\u0026lt;- read.csv2(\u0026quot;dados_linearidade.csv\u0026quot;, header = TRUE, skip = 0)\r# Para achar ajuda/explicação sobre determinada função do R basta digitar ? # seguido da função: ex: ?read.csv2 vai abrir a aba Help no Rstudio mostrando as # definições dos argumentos, etc...\rNeste ponto já podemos ir verificando os dados, se a importação foi correta, etc, para isso simplesmente digitamos o nome do nosso objeto (curva_ex)\ncurva_ex\r## concentração área\r## 1 0.8307923 1148099\r## 2 0.8312060 1179522\r## 3 0.8299649 1155612\r## 4 0.9494770 1313289\r## 5 0.9499498 1298806\r## 6 0.9485314 1309954\r## 7 1.0681616 1481120\r## 8 1.0686935 1485434\r## 9 1.0670978 1469250\r## 10 1.1868462 1643193\r## 11 1.1874372 1640203\r## 12 1.1856642 1625966\r## 13 1.3055308 1795933\r## 14 1.3061809 1807228\r## 15 1.3042306 1815224\r## 16 1.4242154 1980508\r## 17 1.4249246 1963207\r## 18 1.4227970 1955687\r## 19 1.5824616 2195774\r## 20 1.5832496 2186585\r## 21 1.5808856 2186201\rNeste caso vamos trabalhar com uma curva em 7 níveis feita em triplicata, o que corresponde a 21 pontos. A resposta neste exemplo corresponde a área de pico cromatográfico .\nSe quisermos trocar os nomes da colunas para nomes sem carateres especiais (à, ç, ã, …), o que é recomendado, utilizamos a funcão colnames()\ncolnames(curva_ex)\u0026lt;-c(\u0026quot;conc\u0026quot;, \u0026quot;area\u0026quot;)\rcurva_ex\r## conc area\r## 1 0.8307923 1148099\r## 2 0.8312060 1179522\r## 3 0.8299649 1155612\r## 4 0.9494770 1313289\r## 5 0.9499498 1298806\r## 6 0.9485314 1309954\r## 7 1.0681616 1481120\r## 8 1.0686935 1485434\r## 9 1.0670978 1469250\r## 10 1.1868462 1643193\r## 11 1.1874372 1640203\r## 12 1.1856642 1625966\r## 13 1.3055308 1795933\r## 14 1.3061809 1807228\r## 15 1.3042306 1815224\r## 16 1.4242154 1980508\r## 17 1.4249246 1963207\r## 18 1.4227970 1955687\r## 19 1.5824616 2195774\r## 20 1.5832496 2186585\r## 21 1.5808856 2186201\r\r2.3 Ajuste do modelo linear\rO nosso modelo é representado por:\n\\[\rY_{ij}= \\beta_0 + \\beta_1 x_{ij} + \\epsilon_{ij},\\quad j=1,...,n_i \\quad e \\quad i=1,...k\r\\]\nEm que:\n\r\\(Y_{ij}\\) representa o sinal analítico (área, absorbância. etc…);\r\\(x_{ij}\\) representa a concentração;\r\\(\\beta_0\\) representa o coeficiente linear ou intercepto;\r\\(\\beta_1\\) representa o coeficiente angular;\r\\(\\epsilon\\) representa o erro experimental;\r\\(n_i\\) representa o número de réplicas do ponto \\(i\\) de concentração;\r\\(k\\) representa o número de pontos ou níveis.\r\rO ajuste de modelos lineares é feito no R fácilmente com a função lm(). A função faz a regressão e a análise da variância e covariância.\nlin_mod\u0026lt;- lm(area~conc, data=curva_ex)\r# Neste caso vamos fazemos regressão da área (coluna 2) em função da concentração\r# (coluna 1) do objeto curva_ex e deixar os resultados num novo objeto que chamaremos de lin_mod\rlin_mod\r## ## Call:\r## lm(formula = area ~ conc, data = curva_ex)\r## ## Coefficients:\r## (Intercept) conc ## 9714 1375205\rPronto, temos um modelo de regressão simples feito no R, temos que os dados são ajustados por uma curva linear de intercepto= 9713,8749 e coeficiente angular= 1375205,4338 Os resultados dos componentes de modelo podem ser acessados utilizando a função summary():\nsummary(lin_mod)\r## ## Call:\r## lm(formula = area ~ conc, data = curva_ex)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -17284 -6071 -422 4526 26729 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 9714 11135 0,87 0,39 ## conc 1375205 9146 150,36 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0,001 \u0026#39;**\u0026#39; 0,01 \u0026#39;*\u0026#39; 0,05 \u0026#39;.\u0026#39; 0,1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 10300 on 19 degrees of freedom\r## Multiple R-squared: 0,999, Adjusted R-squared: 0,999 ## F-statistic: 2,26e+04 on 1 and 19 DF, p-value: \u0026lt;2e-16\rPodemos criar uma tabela para mostrar os valores ajustados e os residuos:\nobs\u0026lt;-seq(1,length(curva_ex$conc), 1)\rx\u0026lt;-curva_ex$conc\ry\u0026lt;-curva_ex$area\ryhat\u0026lt;-lin_mod$fitted.values\rresiduos\u0026lt;-lin_mod$residuals\rdados_cal\u0026lt;-data.frame(obs,x,y,yhat,residuos)\rcolnames(dados_cal) \u0026lt;- c(\u0026quot;No. Observação\u0026quot;, \u0026quot;Concentração\u0026quot;, \u0026quot;Resposta\u0026quot;, \u0026quot;Resposta ajustada\u0026quot;, \u0026quot;Resíduo\u0026quot;)\rdados_cal %\u0026gt;%\rkable(caption = \u0026quot;Tabela de resultados\u0026quot;, align=rep(\u0026#39;c\u0026#39;, 5))\r\rTable 2.1: Tabela de resultados\r\rNo. Observação\rConcentração\rResposta\rResposta ajustada\rResíduo\r\r\r\r1\r0,8308\r1148099\r1152224\r-4125,0\r\r2\r0,8312\r1179522\r1152793\r26729,1\r\r3\r0,8300\r1155612\r1151086\r4525,8\r\r4\r0,9495\r1313289\r1315440\r-2150,7\r\r5\r0,9499\r1298806\r1316090\r-17283,9\r\r6\r0,9485\r1309954\r1314139\r-4185,4\r\r7\r1,0682\r1481120\r1478655\r2464,5\r\r8\r1,0687\r1485434\r1479387\r6047,0\r\r9\r1,0671\r1469250\r1477193\r-7942,5\r\r10\r1,1868\r1643193\r1641871\r1321,8\r\r11\r1,1874\r1640203\r1642684\r-2481,0\r\r12\r1,1857\r1625966\r1640246\r-14279,7\r\r13\r1,3055\r1795933\r1805087\r-9154,0\r\r14\r1,3062\r1807228\r1805981\r1247,0\r\r15\r1,3042\r1815224\r1803299\r11925,1\r\r16\r1,4242\r1980508\r1968303\r12205,3\r\r17\r1,4249\r1963207\r1969278\r-6071,0\r\r18\r1,4228\r1955687\r1966352\r-10665,1\r\r19\r1,5825\r2195774\r2185924\r9850,3\r\r20\r1,5832\r2186585\r2187007\r-422,3\r\r21\r1,5809\r2186201\r2183756\r2444,7\r\r\r\rUma curva de calibração simples é feita utilizando o pacote ggplot2 (tutorial básico):\n#library(ggplot2)\rlin_plot \u0026lt;- ggplot(lin_mod, aes(x=conc, y=area))+\rgeom_smooth(method = \u0026quot;lm\u0026quot;, se = TRUE, color= \u0026quot;red\u0026quot;,\rsize= 0.5, formula = y~x)+ #se= confidence interval\rgeom_point()+\rtheme_bw()\rlin_plot\rUm gráfico muito mais elaborado, que cumple o “gold standard” da APA para reporte de resultados estatísticos pode ser feito utilizando o pacote ggstatsplot:\nggstatsplot::ggscatterstats(\rdata = curva_ex,\rx = conc,\ry = area,\rxlab = \u0026quot;Concentração\u0026quot;, # label for x axis\rylab = \u0026quot;Resposta analítica\u0026quot;, # label for y axis\rtitle = \u0026quot;Curva de calibração\u0026quot;\r)\r\r2.4 Análise dos coeficientes do modelo linear:\rPrimeiro avaliamos os coeficientes do modelo linear:\n\rTable 2.2: Tabela de coeficientes\r\r\rEstimativa\rDesvio Padrão\rValor t\rPr(\u0026gt;|t|)\r\r\r\rIntercepto\r9714\r11135\r0,8724\r0,3939\r\rConcentração\r1375205\r9146\r150,3594\r0,0000\r\r\r\r2.4.1 Intercepto\rCom os resultados da tabela acima, além das estimativas dos parâmetros, podemos avaliar a significância dos parâmetros por meio do teste \\(t\\). O valor t é calculado dividindo o valor do coeficiente pelo erro padrão. Em relação ao parâmetro intercepto, temos que as hipóteses são dadas por:\n\\(H_0\\) : Intercepto é igual a zero (\\(\\beta_0= 0\\))\n\\(H_1\\) : Intercepto é diferente de zero (\\(\\beta_0 \\neq 0\\))\nO valor \\(t\\) para o intercepto é dado por:\n\\[\rt = \\frac{\r\\hat{\\beta_0}\r}\r{\r\\sqrt{\rVar(\\hat{\\beta_0})\r}\r} = \\frac{\r9713,8749\r}\r{\r11134,7006\r} = 0,8724\r\\]\nno qual \\(\\sqrt{Var(\\hat{\\beta_0})}\\) é o desvio padrão do intercepto dado nos resultados da tabela acima.\nO valor crítico da distribuição \\(t\\) ao nível de significância do 5% (\\(\\alpha=0,05\\)) e \\(n-2\\) graus de liberdade é dado por \\(t_{0,95, 19}= 1,7291\\). Como o valor \\(p\\) associado a esse teste \\(t\\) \\((2\\times Pr(t_{0,95, 19}\u0026gt;|t|)=0,3939)\\) é maior que 0,05 aceitamos \\(H_0\\) e concluimos que o intercepto é significativamente igual a zero ao nível de significância do 5%\nNo R, o valor crítico da distribuição \\(t\\) é calculado pela função qt:\n# nível de significancia\ra= 0.95\r# graus de liberdade (n-2)\rdf= length(curva_ex[,1])-2\r# Valor crítico da distribuição t\rqt(a,df)\r## [1] 1,729\r\r2.4.2 Coeficiente angular\rEm relação ao coeficiente angular, temos que as hipóteses são:\n\\(H_0\\) : Coeficiente angular é igual a zero (\\(\\beta_1 = 0\\))\n\\(H_1\\) : Coeficiente angular é diferente de zero (\\(\\beta_1 \\neq 0\\))\nO estatístico T do teste é dado por:\n\\[\rT = \\frac{\r\\hat{\\beta_1}\r}\r{\r\\sqrt{\rVar(\\hat{\\beta_1})\r}\r}\r= \\frac{\r1375205,4338\r}\r{\r9146,1194\r}\r= 150,3594\r\\]\nO valor crítico da distribuição \\(t\\) ao nível de significância do 5% (\\(\\alpha=0,05\\)) e \\(n-2\\) graus de liberdade é dado por \\(t_{0,95, 19}= 1,7291\\). Como o valor \\(p\\) associado a esse teste \\(t\\) (\\(2\\times Pr(t_{0,95, 19}\u0026gt;|t|)= 1,087\\times 10^{-30}\\)) é menor que 0,05 rejeitamos \\(H_0\\) e concluimos que o coeficiente angular é significativamente diferente de zero ao nível de significância do 5%\n\r2.4.3 Anova no método dos mínimos quadrados ordinários (MMQO)\rAvaliamos também a significância do modelo por meio do teste F da ANOVA. Vale ressaltar que temos um modelo de regressão simples, desta forma o teste F da ANOVA é equivalente ao teste \\(t\\). A seguir, temos a Tabela da ANOVA.\n\rTable 2.3: Tabela de anova\r\r\rGraus de liberdade\rSoma dos quadrados\rQuadrado médio\rvalor-F\rPr(\u0026gt;F)\r\r\r\rConcentração\r1\r2404379417766\r2404379417766\r22608\r0\r\rResíduos\r19\r2020669009\r106351000\rNA\rNA\r\r\r\rNo R o Anova é feito pela função anova.\nanova(lin_mod)\r## Analysis of Variance Table\r## ## Response: area\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## conc 1 2404379417766 2404379417766 22608 \u0026lt;2e-16 ***\r## Residuals 19 2020669009 106351000 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0,001 \u0026#39;**\u0026#39; 0,01 \u0026#39;*\u0026#39; 0,05 \u0026#39;.\u0026#39; 0,1 \u0026#39; \u0026#39; 1\rPara testarmos a significância do coeficiente angular do modelo com o teste F da ANOVA, apresentamos as seguintes hipóteses:\n\\(H_0\\) : Coeficiente angular é igual a zero.\n\\(H_1\\) : Coeficiente angular é diferente de zero.\nA estatística de teste é dada pela divisão do quadrado médio da regressão (QMR) pelo quadrado médio dos resíduos ou erros (QME):\n\\[\r\\begin{eqnarray}\rF_{OBS}= \\frac{QMR}{QME}\r= \\frac{\\frac{SQR}{p-1}}{\\frac{SQE}{n-p}}\r= \\frac{\\frac{\\sum_{i=1}^{n}(\\hat{y_i}-\\bar y)^2}{p-1}}\r{\\frac{\\sum_{i=1}^{n}({y_i}-\\hat{y_i})^2}{n-p}} \\\\\rF_{OBS}= \\frac{\\frac{2,4044\\times 10^{12}}{1}}\r{\\frac{2020669009,2465}{19}}\r= 22607,9624\r\\end{eqnarray}\r\\]\nOnde \\(SQR\\) é a soma dos quadrados da regressão, \\(SQE\\) é a soma dos quadrados dos erros ou resíduos, \\(p\\) é o número de parâmetros do modelo, \\(n\\) é o número total de pontos, \\(\\bar{y}\\) é a média aritmética dos valores de y no centroide, \\(y_i\\) é um valor individual de y obtido experimentalmente em um determinado ponto (iésimo ponto) e \\(\\hat{y_i}\\) é um valor individual de y calculado pelo modelo (equação) em um determinado ponto (i-ésimo ponto).\nA região crítica para o teste F é dada por \\(F_{\\alpha, p-1, n-p}= F_{0.95,1, 19} = 4,3807\\). Como a estatística observada é maior que o quantil da distribuição para a determinação da região crítica (\\(F_{OBS}\\) \u0026gt; \\(F\\)) e o valor \\(p\\) associado a esse teste \\(F\\) (\\(2\\times Pr(F_{0.95,1, 19}\u0026gt;F)= 1,087\\times 10^{-30}\\)) é menor que 0,05 rejeitamos \\(H_0\\) e concluimos que o coeficiente angular é significativamente diferente de zero ao nível de significância do 5%\n\r\r2.5 Análise de Resíduos\rA tabela a seguir, apresenta a análise exploratória dos resíduos.\n\rTable 2.4: Tabela dos resíduos\r\rMínimo\r1Q\rMediana\rMédia\r3Q\rMáximo\r\r\r\r-17284\r-6071\r-422,3\r0\r4526\r26729\r\r\r\rObservando a tabela acima, podemos estudar se os valores de mínimo e máximo, em módulo apresentam ou não uma diferença notável, assim como a mediana e a média, o que nos dá indícios de que a distribuição dos resíduos é simétrica.\nO coeficiente de correlação de Pearson mede o grau de proporcionalidade entre a variável explicativa (concentração) e a varíavel resposta (área). Temos que o coeficiente de determinação R2 é dado pela divisão da soma de quadrados da regressão (SQR) pela soma de quadrados total (SQT=SQR+SQE):\n\\[\r\\begin{eqnarray}\rR^2= \\frac{SQR}{SQT}=\\frac{SQE}{SQR+SQE}\r= \\frac{2,4044\\times 10^{12}}{2,4064\\times 10^{12}}\r= 0,9992 \\\\\rr=\\sqrt{R^2}=\\sqrt{0,9992} = 0,9996\r\\end{eqnarray}\r\\]\rLogo o critério da RDC em relação ao coeficiente é satisfeto, visto que 0,9996 está acima do valor especificado pela agência reguladora (0,990). Note que o coeficiente de determinação representa a relação sinal/ruído, em que SQR está relacionada ao sinal analítico e o ruído está relacionada ao SQT.\n2.5.1 Gráficos de diagnóstico\r\rNo gráfico de residuos vs valores ajustados observamos se existe comportamento não linear, dados dispersos homogenamente no redor de uma linha horizontal sem padrão aparente é um indicador de relação linear.\n\rO gráfico Q-Q indica se os resíduos estão distribuidos normalmente. Se os resíduos seguem a linha reta é um indício de que a suposição de normalidade para os erros experimentais é satisfeita.\n\rO gráfico de scale-location indica se os resíduos estão distribuídos igualmente ao longo dos intervalos dos preditores, uma linea horizontal com pontos dispersos aleatóriamente na volta é um indicativo de que a suposição de homocedasticidade é satisfeita.\n\rO gráfico de distância de Cook indica se existe algum punto influente, alguns indicam uma distância maior do que 1 para considerar um punto influente. Outras referências indicam como limite um valor de 4/n ou 4/(n-k-1), onde n=número de observações e k é o número de variáveis explicativas.\n\rNo gráfico de resíduos vs leverage observamos a dispersão dos resíduos em função do leverage. O leverage é uma medida de quão distantes os valores das variáveis independentes de uma observação estão daqueles das outras observações. O gráfico é utilizado principalmente para detectar heteroscedasticidade e não linearidade. A propagação de resíduos padronizados não deve mudar em função do leverage numa situação de homocedasticidade (variância constante).\n\rO gráfico de distância de cook vs leverage indica se os pontos com alto leverage podem ter influência: ou seja, se excluí-los mudaria muito o modelo. Para isso, podemos observar a distância de Cook, que mede o efeito da exclusão de um ponto no vetor de parâmetro combinado. Uma diretriz aproximada, para tamanhos de amostra grandes, é considerar os valores de distância de Cook acima de 1 para indicar pontos altamente influentes e valores de leverage maiores que 2 vezes o número de preditores dividido pelo tamanho da amostra para indicar observações de alto leverage (no nosso caso esté limite sería \\(2\\times (1/21)=0,0952\\)). Observações de alto leverage são aquelas que possuem valores preditores muito distantes de suas médias, o que pode influenciar muito o modelo ajustado.\n\r\rPara validar as indicações sugeridas a partir da análise gráfica, vamos verificar as hipótese levantadas por meio dos testes estatísticos:\n\r2.5.2 Teste de normalidade dos resíduos\rA seguir, avaliamos a normalidade dos erros experimentais por meio do teste de Shapiro-Wilk, no qual as hipóteses são:\n\\(H_0\\) : A distribuição dos erros experimentais é normal\n\\(H_1\\) : A distribuição dos erros experimentais não é normal\n\rTable 2.5: Teste de normalidade de Shapiro-Wilk\r\rEstatística w\rp-valor\r\r\r\r0,9663\r0,651\r\r\r\rComo resultado temos um valor estatístico w=0,9663 e que o valor \\(p\\) associado a esse teste (0,651) é maior que 0,05, sendo assim, aceitamos a hipótese nula \\(H_0\\) de normalidade dos erros experimentais ao nível de significância de 5%. Note que o resultado do teste de Shapiro-Wilk está em conformidade com a análise gráfica do gráfico Q-Q.\nNo R o teste é realizado pela funçao shapiro.test\nshapiro.test(lin_mod$residuals)\r## ## Shapiro-Wilk normality test\r## ## data: lin_mod$residuals\r## W = 0,97, p-value = 0,7\rOu pela função ols_test_normality do pacote olsrr que determina test adicionais\n#library(olsrr)\rols_test_normality(lin_mod)\r## -----------------------------------------------\r## Test Statistic pvalue ## -----------------------------------------------\r## Shapiro-Wilk 0,9663 0,6510 ## Kolmogorov-Smirnov 0,1174 0,9021 ## Cramer-von Mises 1,7619 0,0000 ## Anderson-Darling 0,2309 0,7758 ## -----------------------------------------------\r\r2.5.3 Teste de homocedasticidade\rA seguir, analisamos a homoscedasticidade por meio do teste de Breusch-Pagan, no qual as hipóteses são:\n\\(H_0\\) : As variâncias são iguais.\n\\(H_1\\) : Pelo menos uma variância difere.\n\rTable 2.6: Teste de homoscedasticidade de Breusch-Pagan\r\rEstatística BP\rp-valor\r\r\r\r1,642\r0,2001\r\r\r\rComo resultado temos um valor estatístico BP=1,6417 e que o valor \\(p\\) associado a esse teste (0,2001) é maior que 0,05, sendo assim, aceitamos a hipótese nula \\(H_0\\) de igualdade das variâncias ao nível de significância de 5%. Note que o resultado do teste está em conformidade com a análise gráfica dos resíduos X valores ajustados. Logo, temos um modelo homocedástico.\nConforme o portal action O teste de Breusch-Pagan é o que melhor se adequa neste caso, visto que assumimos a suposição de normalidade para os erros experimentais. Os teste de Cochran e de Brown-Forsythe não se adequam ao nosso objetivo pois necessitam de grupos e, como os dados do exemplo foram coletados de forma independente, os testes em questão não poderiam ser realizados. Já o teste de Goldfeld-Quandt tem como limitação a exigência de amostras relativamente grandes.\nNo R o teste é realizado pela funçao bptest do pacote lmtest\n#library(lmtest)\rbptest(lin_mod)\r## ## studentized Breusch-Pagan test\r## ## data: lin_mod\r## BP = 1,6, df = 1, p-value = 0,2\r\r2.5.4 Teste de autocorrelação\rPara confirmar que não há dependência das observações, isto é, que não temos sequências de pontos decrescentes ou crescentes vamos aplicar o teste de Durbin-Watson. As hipóteses do teste são:\n\\(H_0\\) : As observações são independentes.\n\\(H_1\\) : As observações não são independentes.\n\rTable 2.7: Teste de autocorrelação de Durbin-Watson\r\rEstatística DW\rp-valor\r\r\r\r1,742\r0,1971\r\r\r\rComo resultado temos um valor estatístico DW=1,7423 e que o valor \\(p\\) associado a esse teste (0,1971) é maior que 0,05, sendo assim, aceitamos a hipótese nula \\(H_0\\) de independência das observações ao nível de significância de 5%.\nNo R o teste é realizado pela funçao dwtest do pacote lmtest\n#library(lmtest)\rdwtest(lin_mod)\r## ## Durbin-Watson test\r## ## data: lin_mod\r## DW = 1,7, p-value = 0,2\r## alternative hypothesis: true autocorrelation is greater than 0\rComo conclusão temos que os critérios da RDC 166 que foram atendidos são:\n\rCoeficiente linear não significativo ao nível de significância de 5%;\rCoeficiente angular significativo ao nível de significância de 5%;\rNormalidade dos erros experimentais;\rIgualdade das variâncias ao nível de significância de 5% (modelo homocedástico);\rAs observações são independentes.\r\r\r\r2.6 Limite de detecção e quantificação\rA partir da análise de linearidade é fácil determinar os limites de detecção e quantificação:\nO limite de detecção é calculado como \\(LOD=3S_r/b\\), onde \\(S_r\\) = desvio padrão dos resíduos e \\(b\\)= coeficiente angular da curva de calibração.\nLOD\u0026lt;-(3*sd(lin_mod$residuals))/(lin_mod$coefficients[[2]])\rLOD\r## [1] 0,02193\rO limite de quantificação é calculado como \\(LOQ=10S_r/b\\), onde \\(S_r\\) = desvio padrão dos resíduos e \\(b\\)= coeficiente angular da curva de calibração.\nLOD\u0026lt;-(3*sd(lin_mod$residuals))/(lin_mod$coefficients[[2]])\rLOD\r## [1] 0,02193\r\r\r3 Detalhes adicionais sobre desenvolvimento do documento dinâmico\r3.1 Equações\rpara trabalhar números num formato conveniente: comma como separador decimal, número adequado de algarismos significativos e notação científica para números muito grandes ou pequenos é necessário ajustar algumas opções do R no começo de documento, ex:\n#### OPÇÕES ######\roptions(OutDec= \u0026quot;,\u0026quot;)\roptions(scipen=6)\roptions(digits=4)\rNa seção 2.4.1 mostramos uma fórmula com resultados que define o valor estatistico t como a razão entre o intercepto e o desvio padrão deste, num relatório automátizado é claro que estes valores são variáveis.\n\\[\rt = \\frac{\r\\hat{\\beta_0}\r}\r{\r\\sqrt{\rVar(\\hat{\\beta_0})\r}\r} = \\frac{\r9713,8749\r}\r{\r11134,7006\r} = 0,8724\r\\]\nPoderiamos fazer esta parte semi-automáticamente digitando os valores encontrados no objeto de clase “lm” (summary(lin_mod) permite acessar os componentes do modelo linear):\nPorém esse não é nosso objetivo, para “brincar” com os valores produzidos no R é precisso conhecer a estrutura dos objetos, para isso utilizamos a função str()\nclass(lin_mod)\r## [1] \u0026quot;lm\u0026quot;\rstr(summary(lin_mod))\r## List of 11\r## $ call : language lm(formula = area ~ conc, data = curva_ex)\r## $ terms :Classes \u0026#39;terms\u0026#39;, \u0026#39;formula\u0026#39; language area ~ conc\r## .. ..- attr(*, \u0026quot;variables\u0026quot;)= language list(area, conc)\r## .. ..- attr(*, \u0026quot;factors\u0026quot;)= int [1:2, 1] 0 1\r## .. .. ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2\r## .. .. .. ..$ : chr [1:2] \u0026quot;area\u0026quot; \u0026quot;conc\u0026quot;\r## .. .. .. ..$ : chr \u0026quot;conc\u0026quot;\r## .. ..- attr(*, \u0026quot;term.labels\u0026quot;)= chr \u0026quot;conc\u0026quot;\r## .. ..- attr(*, \u0026quot;order\u0026quot;)= int 1\r## .. ..- attr(*, \u0026quot;intercept\u0026quot;)= int 1\r## .. ..- attr(*, \u0026quot;response\u0026quot;)= int 1\r## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## .. ..- attr(*, \u0026quot;predvars\u0026quot;)= language list(area, conc)\r## .. ..- attr(*, \u0026quot;dataClasses\u0026quot;)= Named chr [1:2] \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot;\r## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:2] \u0026quot;area\u0026quot; \u0026quot;conc\u0026quot;\r## $ residuals : Named num [1:21] -4125 26729 4526 -2151 -17284 ...\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:21] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ...\r## $ coefficients : num [1:2, 1:4] 9713,875 1375205,434 11134,701 9146,119 0,872 ...\r## ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2\r## .. ..$ : chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;conc\u0026quot;\r## .. ..$ : chr [1:4] \u0026quot;Estimate\u0026quot; \u0026quot;Std. Error\u0026quot; \u0026quot;t value\u0026quot; \u0026quot;Pr(\u0026gt;|t|)\u0026quot;\r## $ aliased : Named logi [1:2] FALSE FALSE\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;conc\u0026quot;\r## $ sigma : num 10313\r## $ df : int [1:3] 2 19 2\r## $ r.squared : num 0,999\r## $ adj.r.squared: num 0,999\r## $ fstatistic : Named num [1:3] 22608 1 19\r## ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:3] \u0026quot;value\u0026quot; \u0026quot;numdf\u0026quot; \u0026quot;dendf\u0026quot;\r## $ cov.unscaled : num [1:2, 1:2] 1,166 -0,938 -0,938 0,787\r## ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2\r## .. ..$ : chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;conc\u0026quot;\r## .. ..$ : chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;conc\u0026quot;\r## - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;summary.lm\u0026quot;\rTemos que o objeto é uma lista de 11 componentes, neste caso estamos interessados no componente que possui os coefficientes:\nsummary(lin_mod)$coefficients\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) 9714 11135 0,8724 3,939e-01\r## conc 1375205 9146 150,3594 1,087e-30\rPara extrair um valor deste data frame temos que indexar o valor que queremos do vetor (objeto[linhas,colunas])\n# Extrair intercepto:\rsummary(lin_mod)$coefficients[1,1]\r## [1] 9714\r# Extrair desvio padrão:\rsummary(lin_mod)$coefficients[1,2]\r## [1] 11135\r# Extrair estatistico t:\rsummary(lin_mod)$coefficients[1,3]\r## [1] 0,8724\r# Também é posível calcular este valor t pois conhecemos a fórmula:\rsummary(lin_mod)$coefficients[1,1]/summary(lin_mod)$coefficients[1,2]\r## [1] 0,8724\rAs equações tem como base o \\(\\LaTeX\\), começamos com uma fórmula simples que não contém código R,\n$$\rT = \\frac{\\hat{\\beta_0}}{\\sqrt{Var(\\hat{\\beta_0})}}= \\frac{a}{b}=c\r$$\rproduz:\n\\[\rT = \\frac{\\hat{\\beta_{0}}}{\\sqrt{Var(\\hat{\\beta_0})}} = \\frac{a}{b} = c\r\\]\nAgora, \\(a\\) corresponde a summary(lin_mod)$coefficients[1,1], \\(b\\) corresponde a summary(lin_mod)$coefficients[1,2] e \\(c\\) corresponde a summary(lin_mod)$coefficients[1,3] ou também \\(c\\)=(summary(lin_mod)$coefficients[1,1])/(summary(lin_mod)$coefficients[1,2]).\nMisturando então \\(\\LaTeX\\) e código R conseguimos a equação que queremos mostrar,\n$$\rt = \\frac{\r\\hat{\\beta_0}\r}\r{\r\\sqrt{\rVar(\\hat{\\beta_0})\r}\r} = \\frac{\r\u0026#39;r summary(lin_mod)$coefficients[1,1]\u0026#39;\r}\r{\r\u0026#39;r summary(lin_mod)$coefficients[1,2]\u0026#39;\r} = \u0026#39;r summary(lin_mod)$coefficients[1,3]\u0026#39;\r$$\rProduz\n\\[\rt = \\frac{\r\\hat{\\beta_0}\r}\r{\r\\sqrt{\rVar(\\hat{\\beta_0})\r}\r} = \\frac{\r9713,8749\r}\r{\r11134,7006\r} = 0,8724\r\\]\n\r3.2 Texto\rNa seção 2.5.2 apresentei o teste de normalidade e o como fazer ele no R, em esse e nos outros testes para resíduos é possível criar uma sequência de análise que no final permite criar vetores de texto com as “conclusões” que são possíveis no teste: aceitar ou não a hipótese nula:\n# teste de shapiro wilk\rsw\u0026lt;-shapiro.test(lin_mod$residuals)\r# O objeto sw possui a classe \u0026quot;htest\u0026quot;, logo precissamos tirar dele o que interessa:\r# a estatistica w e o valor p (sw_p)\rw\u0026lt;-sw$statistic\rsw_p\u0026lt;-sw$p.value\r# Criamos um data frame com a função data.frame\rsw_df\u0026lt;-data.frame(w, sw_p, row.names = NULL)\r# trocamos os nomes das colunas com a função colnames:\rcolnames(sw_df) \u0026lt;- c(\u0026quot;Estatística w\u0026quot;, \u0026quot;p-valor\u0026quot;)\r# Criamos uma tabela para deixar no relatório com os resultados do teste:\rsw_df %\u0026gt;% kable(caption = \u0026quot;Teste de normalidade de Shapiro-Wilk\u0026quot;, align=rep(\u0026#39;c\u0026#39;, 2))\r\r(#tab:shapiro_ex)Teste de normalidade de Shapiro-Wilk\r\rEstatística w\rp-valor\r\r\r\r0,9663\r0,651\r\r\r\r## Parte lógica\rp_crit_sw \u0026lt;-0.05 # Valor crítico de p segundo a RDC 166\rif(sw_p\u0026lt; p_crit_sw){ # se p-valor do teste é menor que o p crítico (0.05)\rvetor_sw\u0026lt;-c(\u0026quot;menor\u0026quot;, \u0026quot;rejeitamos\u0026quot;) # o vetor vetor_sw vai conter as palabras menor e regeitamos\r} else { # se p-valor do teste é maior que o p crítico (0.05)\rvetor_sw\u0026lt;-c(\u0026quot;maior\u0026quot;, \u0026quot;aceitamos\u0026quot;) # o vetor vetor_sw vai conter as palabras maior e aceitamos\r}\rAssim, com esse vetor de palavaras criado podemos dinamizar o texto:\nComo resultado temos um valor estatístico w=sw$statistic e que o valor \\(p\\) associado a esse teste (sw$p.value) é vetor_sw[1] que 0,05, sendo assim, vetor_sw[2] a hipótese nula \\(H_0\\) de normalidade dos erros experimentais ao nível de significância de 5%. Note que o resultado do teste de Shapiro-Wilk está em conformidade com a análise gráfica do gráfico Q-Q.\nProduz:\nComo resultado temos um valor estatístico w=0,9663 e que o valor \\(p\\) associado a esse teste (0,651) é maior que 0,05, sendo assim, aceitamos a hipótese nula \\(H_0\\) de normalidade dos erros experimentais ao nível de significância de 5%. Note que o resultado do teste de Shapiro-Wilk está em conformidade com a análise gráfica do gráfico Q-Q.\n\r3.3 Gráficos\rEsta é uma parte complexa quando se quer personalizar e formatar ao gosto os gráficos, neste caso eu atualizei uma função criada por Raju Rimal que permite a criação de 6 gráficos de diagnóstico personalizados:\n#https://rpubs.com/therimalaya/43190 (modificado 12/12/20)\rdiagPlot\u0026lt;-function(model){\rp1\u0026lt;-ggplot(model, aes(.fitted, .resid))+geom_point()\rp1\u0026lt;-p1+stat_smooth(method=\u0026quot;loess\u0026quot;)+geom_hline(yintercept=0, col=\u0026quot;red\u0026quot;, linetype=\u0026quot;dashed\u0026quot;)\rp1\u0026lt;-p1+xlab(\u0026quot;Valores ajustados\u0026quot;)+ylab(\u0026quot;Residuos\u0026quot;)\rp1\u0026lt;-p1+ggtitle(\u0026quot;Residuos vs Valores ajustados\u0026quot;)+theme_bw()\rp2\u0026lt;-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)\rp2\u0026lt;-p2+geom_qq_line(aes(sample=.stdresid))+xlab(\u0026quot;Quantis teóricos\u0026quot;)+ylab(\u0026quot;Resíduos padronizados\u0026quot;)\rp2\u0026lt;-p2+ggtitle(\u0026quot;Gráfico Q-Q\u0026quot;)+theme_bw()\rp3\u0026lt;-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)\rp3\u0026lt;-p3+stat_smooth(method=\u0026quot;loess\u0026quot;, na.rm = TRUE)+xlab(\u0026quot;Valores ajustados\u0026quot;)\rp3\u0026lt;-p3+ylab(expression(sqrt(\u0026quot;|Resíduos padronizados|\u0026quot;)))\rp3\u0026lt;-p3+ggtitle(\u0026quot;Scale-Location\u0026quot;)+theme_bw()\rp4\u0026lt;-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat=\u0026quot;identity\u0026quot;, position=\u0026quot;identity\u0026quot;)\rp4\u0026lt;-p4+xlab(\u0026quot;Número da observação\u0026quot;)+ylab(\u0026quot;Distância de Cook\u0026quot;)\rp4\u0026lt;-p4+ggtitle(\u0026quot;Distância de Cook\u0026quot;)+theme_bw()\rp5\u0026lt;-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)\rp5\u0026lt;-p5+stat_smooth(method=\u0026quot;loess\u0026quot;, na.rm=TRUE)\rp5\u0026lt;-p5+xlab(\u0026quot;Leverage\u0026quot;)+ylab(\u0026quot;Resíduos padronizados\u0026quot;)\rp5\u0026lt;-p5+ggtitle(\u0026quot;Resíduos vs Leverage\u0026quot;)\rp5\u0026lt;-p5+scale_size_continuous(\u0026quot;Distância de Cook\u0026quot;, range=c(1,5))\rp5\u0026lt;-p5+theme_bw()+theme(legend.position=\u0026quot;bottom\u0026quot;)\rp6\u0026lt;-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method=\u0026quot;loess\u0026quot;, na.rm=TRUE)\rp6\u0026lt;-p6+xlab(\u0026quot;Leverage hii\u0026quot;)+ylab(\u0026quot;Distância de Cook\u0026quot;)\rp6\u0026lt;-p6+ggtitle(\u0026quot;Distância de Cook vs Leverage hii/(1-hii)\u0026quot;)\rp6\u0026lt;-p6+geom_abline(slope=seq(0,3,0.5), color=\u0026quot;gray\u0026quot;, linetype=\u0026quot;dashed\u0026quot;)\rp6\u0026lt;-p6+theme_bw()\rreturn(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, cdPlot=p4, rvlevPlot=p5, cvlPlot=p6))\r}\rCriada a função diagPlot podemos aplicar ela a nosso modelo linear lin_mod e criar o objeto diagPlts que contém os 6 gráficos numa lista, com a função grid.arrange do pacote gridExtra podemos plotar os objetos dessa lista como quisermos, podemos imprimir todos os 6 ou menos e num arranjo diferente:\ndiagPlts\u0026lt;-diagPlot(lin_mod)\rclass(diagPlts)\r#library(grid)\r#library(gridExtra)\rdo.call(grid.arrange, c(diagPlts, top=\u0026quot;Gráficos de diagnóstico de resíduos\u0026quot;, ncol=1))\rParece díficil e trabalhoso más com prática vai ficando cada día mais fácil, o bom é que esse trabalho só se faz uma vez, quando se tem que fazer o mesmo tipo de relatório várias vezes (exemplo relatorio de linearidade/validação) este trabalho se vê recompensado ao fazer um documento com este tipo de detalhes de forma rápida, reprodutível e livre de erros, pois todo é recalculado e diagramado de novo em poucos segundos.\n\r\r","date":1608163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608163200,"objectID":"70f942f3b8b222be7043750d0361bbb2","permalink":"/post/linearidade1/","publishdate":"2020-12-17T00:00:00Z","relpermalink":"/post/linearidade1/","section":"post","summary":"Automatização da análise da Linearidade da validação analítica","tags":["R Markdown","Linearidade","automatização","validação","R","documento dinâmico"],"title":"Geração automática de relatórios analíticos 1.  Linearidade conforme RDC 166","type":"post"},{"authors":["Alejandro Yopasá Arenas"],"categories":["Validação"],"content":"\rCom base nos resultados de milhares de estudos de desempenho de métodos interlaboratoriais, o Dr. William Horwitz (FDA, AOAC) calculou as estimativas dos desvios padrão de repetibilidade e reprodutibilidade (entre laboratórios). Ele notou que à medida que a concentração do analito diminuía em duas ordens de magnitude, o desvio padrão relativo da reprodutibilidade (RSDR) aumentava por um fator de dois. Este padrão persistiu pelo menos até níveis sub-ppm. Essas descobertas deram origem à famosa ‘Trumpete de Horwitz’, que retrata a relação, expressa como um intervalo de confiança, do desvio padrão da reprodutibilidade prevista \\(\\sigma_R\\) em função da concentração \\(C\\), sendo esta expressa como razão de massa (fração decimal), \\(\\sigma_R\\) é expresso por:\n\\[\r\\sigma_R = 0.02 C^{0.8495} \\qquad ou \\qquad log_{10} \\sigma_R = 0.8495 log_{10} C -1.6990\r\\]\nO desvio padrão relativo previsto expresso como porcentagem (%), é calculado como:\n\\[\rPRSD_R(\\%)= \\frac{\\sigma_R}{C} \\times 100 \\qquad então \\qquad PRSD_R(\\%)= 2 \\times C^{0.8495}C^{-1} = 2C^{-0.1505}\r\\]\nNas diretrizes para requisitos de métodos padrão da AOAC, este PRSDR é uma medida da reprodutibilidade de dados entre laboratórios. A repetibilidade, ou valores alvo preditos aceitáveis em ensaios intralaboratório para repetibilidade (PRSD r) são calculados como \\(PRSD_r (\\%) = PRSD_R (\\%) / 2\\), que representa o melhor caso.\nCálculos\rPara calcular e relatar parâmetros estatísticos, os dados podem ser expressos em quaisquer unidade conveniente (por exemplo: %, ppm, ppb, mg g-1, μg g-1; μg kg-1; μg L-1, μg μL-1, etc.). Para relatar valores HorRat, ou para cálculo da PRSD(R) as concentrações devem ser expressas como uma fração de massa, onde as unidades do numerador e denominador são as mesmas, por exemplo:\n\\[\r1 \\%= \\frac{1 g}{100 g}= 0.01; \\qquad \\qquad 1 ppm=\\frac{1 mg}{1 kg}=\\frac{1 mg}{1000000 mg}= 0.000001\r\\]\nPodemos criar uma função R para calcular o PRSDR da seguinte forma:\nPRSDR\u0026lt;-function(conc, unit){\r# Convertendo em unidades de razão de massa\rif (unit== \u0026quot;razao_massa\u0026quot;) {\rmr=conc\r} else if (unit== \u0026quot;porcentagem\u0026quot;) {\rmr=conc/100\r} else if (unit== \u0026quot;%\u0026quot;) {\rmr=conc/100\r} else if (unit== \u0026quot;ppm\u0026quot;) {\rmr=conc/1000000\r} else if (unit== \u0026quot;ppb\u0026quot;) {\rmr=conc/1000000000\r} else if (unit== \u0026quot;ppt\u0026quot;) {\rmr=conc/1000000000000\r} else {\rwarning(\u0026quot;por favor definir a unidade: razao_massa, %, porcentagem, ppm, ppb,\rppt\u0026quot;)\r}\r# Calculando o desvio padrão da reprodutibilidade de Horwitz\r2*(mr^-0.1505)\r}\rAssim, usando a fórmula, podemos representar graficamente a curva de Horwitz da concentração de um material puro (100 %, razão de massa = 1) até uma faixa de concentração de, por exemplo, traços de poluentes (1 ppb, razão de massa =r 1/1e9), a concentração em x é expressa como logaritmo.\nconc \u0026lt;- seq(from = log10(1e-9), to = log10(1), length.out = 100)\rPRSDR_R \u0026lt;- PRSDR(10^(conc), \u0026quot;razao_massa\u0026quot;)\rHor_tru \u0026lt;- data.frame(conc, PRSDR_R)\rlibrary(ggplot2)\revents \u0026lt;- data.frame(conc2 = c(log10(1),\rlog10(0.1),\rlog10(0.01),\rlog10(0.001),\rlog10(100/1e6),\rlog10(10/1e6),\rlog10(1/1e6),\rlog10(100/1e9),\rlog10(10/1e9),\rlog10(1/1e9)),\rtext = c(\u0026#39;100 %\u0026#39;,\r\u0026quot;10 %\u0026quot;,\r\u0026#39;1 %\u0026#39;,\r\u0026quot;0.1%\u0026quot;,\r\u0026quot;100 ppm\u0026quot;,\r\u0026quot;10 ppm\u0026quot;,\r\u0026quot;1 ppm\u0026quot;,\r\u0026quot;100 ppb\u0026quot;,\r\u0026quot;10 ppb\u0026quot;,\r\u0026quot;1 ppb\u0026quot;))\rp\u0026lt;-ggplot(Hor_tru, aes(x=conc, y=PRSDR_R))+\rgeom_line(aes(color=\u0026quot;Reprodutibilidade\u0026quot;))+\rgeom_line(aes(y=-PRSDR_R, color=\u0026quot;Reprodutibilidade\u0026quot;))+\rgeom_line(aes(y= PRSDR_R/2, color=\u0026quot;Repetibilidade\u0026quot;))+\rgeom_line(aes(y=-PRSDR_R/2, color=\u0026quot;Repetibilidade\u0026quot;))+\rgeom_vline(data = events, aes(xintercept = conc2), linetype=2, color=\u0026quot;gray70\u0026quot;)+\rgeom_text(data = events, mapping = aes(label = text, x=conc2, y=-47), angle = 90, hjust = 0, vjust= 1)+\rxlab(expression(Log[10](razão~de~massa)))+\rylab(expression(Desvio~padrão~relativo~predito~(\u0026quot;%\u0026quot;)))+\rtheme_light()+\rtheme(legend.title = element_blank())\rp\rA função de Horwitz mostra que o desvio esperado numa precisão depende da concentração de trabalho, para uma análise típico de teor um desvio de uns 2% é aceitável mas também sería aceitável um 40% de desvio numa análise de contaminantes emergentes (sub-ppb) feito por lanboratórios diferentes. A curva de Horwitz é útil como estimativa inicial da variabilidade esperada entre laboratórios antes da realização de um estudo interlaboratorial.\n\r","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"387a203eb2c0496be6e1240acc47a995","permalink":"/post/horwitz/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/post/horwitz/","section":"post","summary":"A fascinante função de Horwitz","tags":["R Markdown","Horwitz","Precisão","validação"],"title":"Predição do Desvio Padrão Relativo da Reprodutibilidade em Validação","type":"post"},{"authors":["Alejandro Yopasá Arenas"],"categories":["Software"],"content":"\r\rFerramentas básicas utilizadas na construção deste site/blog\rFaz muitos anos venho trabalhando num único ambiente integrado (RStudio) que me ajuda nos diversos desafíos encontrados na carreira de cientista, desde o básico como fazer um teste estatístico, como fazer um artigo reprodutível com foco em ciencia de dados, capítulos inteiros de uma tese de doutorado e incluso este site, que está sendo feito desde o RStudio/Hugo.\nFerramentas básicas:\nR é a base de tudo o meu trabalho, R é um ambiente de software livre para computação estatística.\n\rO IDE RStudio é um conjunto de ferramentas integradas projetadas para ajudar a ser mais produtivo com R e Python. Inclui um console, um editor que oferece suporte à execução direta de código e uma variedade de ferramentas robustas para graficar, depurar e gerenciar o espaço de trabalho.\n\rO R Markdown fornece uma estrutura para autores em ciência de dados. Num único arquivo R Markdown é possível salvar e executar código assim como gerar relatórios de alta qualidade que podem ser compartilhados com um público em diversos formatos. R Markdown é uma variante do Markdown que tem blocos de código R incorporados, para serem usados com o knitr para facilitar a criação de relatórios reproduzíveis baseados na web. A sintaxe Markdown tem alguns aprimoramentos; por exemplo, você pode incluir equações em \\(\\LaTeX{}\\).\n\rO ggplot2 é um sistema para a criação declarativa de gráficos, baseado na Gramática dos Gráficos. É difícil descrever como o ggplot2 funciona porque ele incorpora uma profunda filosofia da visualização, porém é uma das ferramentas mais interessantes para geração de gráficos complexos. No ggplot você fornece os dados, diz ao ggplot2 como mapear as variáveis para a estética, qual base gráfica utilizar e então adiciona camadas, escalas, especificações de facetas e sistemas de coordenadas. O ggplot2 faz parte do tidyverse que é uma coleção de pacotes do R projetados para ciência de dados. Todos os pacotes compartilham uma filosofia de design, gramática e estruturas de dados subjacentes.\n\r\r\rInstalação dos softwares\rInstalar R no Windows\rIr a https://cloud.r-project.org/bin/windows/base/\rClick no link “Download R 4.0.3 for Windows” (ou a versão que estiver como mais recente)\rBaixar o arquivo e dar doble clique para instalar, para manter as configurações predefinidas é só dar clique em “next” até finalizar o processo.\r\rPara outros sistemas operativos as distribuições do R se encontram em https://cloud.r-project.org\n\rInstalação do Rstudio\rIr a https://rstudio.com/products/rstudio/download/#download\rSelecionar o instalador apropiado dependendo do sistema operativo\rBaixar o arquivo e dar doble clique para instalar, para manter as configurações predefinidas é só dar clique em “next” até finalizar o processo.\r\rÁrea de trabalho típica no RStudio\n\r\rInstalação do Tidyverse\rO tidyverse é um conjunto de pacotes que trabalham em armonia fazendo mais mais fácil a instalação é o carregamento em comandos simples.\n# Instalação desde o CRAN (The Comprehensive R Archive Network)\rinstall.packages(\u0026quot;tidyverse\u0026quot;)\r# Ou para instaçar a versão em desenvolvimento desde o GitHub\r# install.packages(\u0026quot;devtools\u0026quot;)\rdevtools::install_github(\u0026quot;tidyverse/tidyverse\u0026quot;)\rUtilizando o comando library(tidyverse) vão ser carregados os pacotes da base tidyverse:\n\rggplot2, para visualização de dados.\rdplyr, para manipulação de dados.\rtidyr, para arranjo de dados.\rreadr, para importar dados.\rpurrr, para programação funcional.\rtibble, para tibbles, uma reimaginação dos data frames.\rstringr, para cadeias de caracteres.\rforcats, para fatores.\r\r\r\r","date":1607126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607126400,"objectID":"9e301c59dfe3cf5a52fb9b8e755db42b","permalink":"/post/rstudio-r/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/post/rstudio-r/","section":"post","summary":"Guia básico de R, RStudio e RMarkdown","tags":["R","R Studio","tidyverse","Software"],"title":"Introdução e ferramentas de trabalho","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Alejandro Yopasá Arenas","Anne Hélène Fostier"],"categories":null,"content":" Click the Cite button above to import publication metadata into a reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   --\rA series of raster and vector files that allow the visualization of the results presented in the manuscript are shared as a Mendeley Dataset.\n","date":1530057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530057600,"objectID":"406a4dd9d5050888a924383c6a758efb","permalink":"/publication/journal-article/sig-2018/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/sig-2018/","section":"publication","summary":"The model uses relative simple and available data from official sources that were never previously used together for this purpose. Although simple, the qualitative approach based on worst-case scenarios can be used in future research as a way to define possible experimental sites where quantitative analysis could be performed for a better and real risk assessment.","tags":["Química Ambiental"],"title":"Exposure of Brazilian soil and groundwater to pollution by coccidiostats and antimicrobial agents used as growth promoters","type":"publication"},{"authors":["Alejandro Yopasá Arenas","Gustavo de Souza Pessôa","Marco Aurélio Zezzi Arruda","Anne Hélène Fostier"],"categories":null,"content":" Click the Cite button above to import publication metadata into a reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   --\r","date":1512432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512432000,"objectID":"31eb724ed9d4aa8e8092741750aa421c","permalink":"/publication/journal-article/pvp-agnp/","publishdate":"2017-12-05T00:00:00Z","relpermalink":"/publication/journal-article/pvp-agnp/","section":"publication","summary":"Transport of silver nanoparticles is studied for the first time in tropical soils.","tags":["Química Ambiental"],"title":"Mobility of polivinylpyrrolidone coated silver nanoparticles in tropical soils","type":"publication"},{"authors":["Odilon França de Oliveira Neto","Alejandro Yopasá Arenas","Anne Hélène Fostier"],"categories":null,"content":"\r Click the Cite button above to import publication metadata into a reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   --\r","date":1496016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496016000,"objectID":"d113722b71b2bdcdc4a546ca42b3b478","permalink":"/publication/journal-article/thiabendazole/","publishdate":"2017-12-05T00:00:00Z","relpermalink":"/publication/journal-article/thiabendazole/","section":"publication","summary":"Thiabendazole (TBZ) is an ionizable anthelmintic agent that belongs to the class of benzimidazoles. It is widely used in veterinary medicine and as a fungicide in agriculture. Sorption and desorption are important processes influencing transport, transformation, and bioavailability of xenobiotic compounds in soils; data related to sorption capacity are therefore needed for environmental risk assessments. The aim of this work was to assess the sorption potential of TBZ in four Brazilians soils (sandy, sandy-clay, and clay soils), using batch equilibrium experiments at three pH ranges (2.3–3.0, 3.8–4.2, and 5.5–5.7). The Freundlich sorption coefficient (KF) ranged from 9.0 to 58 μg$^{1–1/n}$ (mL)$^{1/n}$ g$^{−1}$, with higher values generally observed at the lower pH ranges (2.3–3.0 and 3.8–4.2) and for clay soils. The highest organic carbon-normalized sorption coefficients (K$_{OC}$) obtained at pH 3.8–5.7 (around the natural pH range of 4.1–5.0) for both clay soils and sandy-clay soil were 3255 and 2015 mL g$^{−1}$, respectively. The highest correlations K$_F$ vs SOM (r = 0.70) and K$_F$ vs clay content (r = 0.91) were observed at pH 3.8–4.2. Our results suggest that TBZ sorption/desorption is strongly pH dependent and that its mobility could be higher in the studied soils than previously reported in soils from temperate regions.","tags":["Química Ambiental"],"title":"Sorption of thiabendazole in sub-tropical Brazilian soils","type":"publication"}]